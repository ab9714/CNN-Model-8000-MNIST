{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Target: adjust optimizer and scheduler for better accuracy\n",
        "\n",
        "Results:\n",
        "\n",
        "Parameters: 5710\n",
        "\n",
        "Best Train Accuracy: 98.26\n",
        "\n",
        "\n",
        "Best Test Accuracy: 99.41\n",
        "\n",
        "Analysis: 99.4 hit two times epoch 14 and 15\n",
        "\n"
      ],
      "metadata": {
        "id": "93C69xKHL_SK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boQ-eVggsY18",
        "outputId": "012833e9-5c56-428d-c0ec-9e09cf7d4493"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "           Dropout-4           [-1, 10, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           1,440\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "           Dropout-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9           [-1, 10, 24, 24]             160\n",
            "        MaxPool2d-10           [-1, 10, 12, 12]               0\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,440\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "          Dropout-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-16             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 16, 8, 8]              32\n",
            "          Dropout-18             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-19             [-1, 16, 1, 1]               0\n",
            "           Conv2d-20             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 5,710\n",
            "Trainable params: 5,710\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.62\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.65\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=0 Loss=0.3213 Batch=468 Accuracy=71.52: 100%|██████████| 469/469 [00:42<00:00, 10.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1619, Accuracy: 9492/10000 (94.92%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=1 Loss=0.2188 Batch=468 Accuracy=93.85: 100%|██████████| 469/469 [00:40<00:00, 11.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0722, Accuracy: 9766/10000 (97.66%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=2 Loss=0.1530 Batch=468 Accuracy=95.71: 100%|██████████| 469/469 [00:39<00:00, 11.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0407, Accuracy: 9857/10000 (98.57%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=3 Loss=0.1159 Batch=468 Accuracy=96.22: 100%|██████████| 469/469 [00:40<00:00, 11.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0479, Accuracy: 9838/10000 (98.38%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=4 Loss=0.0442 Batch=468 Accuracy=96.61: 100%|██████████| 469/469 [00:41<00:00, 11.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0506, Accuracy: 9851/10000 (98.51%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=5 Loss=0.1623 Batch=468 Accuracy=96.90: 100%|██████████| 469/469 [00:41<00:00, 11.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0306, Accuracy: 9899/10000 (98.99%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=6 Loss=0.0562 Batch=468 Accuracy=97.13: 100%|██████████| 469/469 [00:41<00:00, 11.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0286, Accuracy: 9907/10000 (99.07%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=7 Loss=0.0877 Batch=468 Accuracy=97.33: 100%|██████████| 469/469 [00:42<00:00, 11.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0291, Accuracy: 9900/10000 (99.00%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=8 Loss=0.0536 Batch=468 Accuracy=97.42: 100%|██████████| 469/469 [00:40<00:00, 11.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0251, Accuracy: 9921/10000 (99.21%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=9 Loss=0.0302 Batch=468 Accuracy=97.69: 100%|██████████| 469/469 [00:41<00:00, 11.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9927/10000 (99.27%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=10 Loss=0.0809 Batch=468 Accuracy=97.93: 100%|██████████| 469/469 [00:41<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9925/10000 (99.25%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=11 Loss=0.0345 Batch=468 Accuracy=98.02: 100%|██████████| 469/469 [00:41<00:00, 11.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0196, Accuracy: 9937/10000 (99.37%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=12 Loss=0.0423 Batch=468 Accuracy=98.21: 100%|██████████| 469/469 [00:41<00:00, 11.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0190, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=13 Loss=0.0635 Batch=468 Accuracy=98.21: 100%|██████████| 469/469 [00:41<00:00, 11.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9940/10000 (99.40%)\n",
            "\n",
            "Times achieved ≥99.4%: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=14 Loss=0.0924 Batch=468 Accuracy=98.26: 100%|██████████| 469/469 [00:42<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0179, Accuracy: 9941/10000 (99.41%)\n",
            "\n",
            "Times achieved ≥99.4%: 2\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Initial Block: 28x28x1 -> 26x26x10\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(0.005)  # Slightly increased dropout\n",
        "        )\n",
        "\n",
        "        # Conv Block 1: 26x26x10 -> 24x24x16\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        # Transition Block: 24x24x16 -> 12x12x10\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, kernel_size=1, bias=False),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Conv Block 2: 12x12x10 -> 10x10x16\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        # Conv Block 3: 10x10x16 -> 8x8x16\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=8)\n",
        "        )\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, kernel_size=1, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.conv5(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(data)\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Epoch={epoch} Loss={loss.item():.4f} Batch={batch_idx} Accuracy={100*correct/processed:.2f}')\n",
        "    return 100 * correct / processed\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100.*correct/len(test_loader.dataset):.2f}%)\\n')\n",
        "    return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(1)\n",
        "    batch_size = 128\n",
        "    epochs = 15\n",
        "\n",
        "    # Enhanced data augmentation with more aggressive transformations\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomRotation((-12.0, 12.0)),  # Increased rotation\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.13, 0.13),  # Increased translation\n",
        "            scale=(0.82, 1.18),      # Increased scale range\n",
        "            shear=(-3, 3)            # Added shear\n",
        "        ),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.15,          # Increased brightness adjustment\n",
        "            contrast=0.15            # Increased contrast adjustment\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1))  # Added random erasing\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True, transform=train_transform),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=test_transform),\n",
        "        batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Net().to(device)\n",
        "\n",
        "    # Modified optimizer settings\n",
        "    optimizer = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=0.01,\n",
        "        weight_decay=2e-5,  # Increased weight decay\n",
        "        betas=(0.9, 0.99)   # Modified beta parameters\n",
        "    )\n",
        "\n",
        "    # Modified learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=0.018,        # Increased max learning rate\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.12,      # Shorter warmup\n",
        "        div_factor=20,       # More aggressive initial LR division\n",
        "        final_div_factor=150 # More aggressive final LR division\n",
        "    )\n",
        "\n",
        "    summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "    accuracies_above_99_4 = 0\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_acc = train(model, device, train_loader, optimizer, scheduler, epoch)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        if test_acc >= 99.4:\n",
        "            accuracies_above_99_4 += 1\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Times achieved ≥99.4%: {accuracies_above_99_4}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SiggidWrL5RJ"
      }
    }
  ]
}