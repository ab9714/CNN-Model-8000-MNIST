{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNzvlQJgnh6Dle5kL6EF5Eq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Target: Reduce Dropout Rate , optimize train transforms through data augmentation and change optimizer\n",
        "\n",
        "Results:\n",
        "\n",
        "Parameters: 5710\n",
        "\n",
        "Best Train Accuracy: 98.12\n",
        "\n",
        "\n",
        "Best Test Accuracy: 99.36\n",
        "\n",
        "Analysis: better convergence but still accuracy below 99.4%"
      ],
      "metadata": {
        "id": "_XPKIKXlDVyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7TAYxDzAi6b",
        "outputId": "e24a0764-5d99-4b40-bb84-098281c824a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 10, 26, 26]              90\n",
            "              ReLU-2           [-1, 10, 26, 26]               0\n",
            "       BatchNorm2d-3           [-1, 10, 26, 26]              20\n",
            "           Dropout-4           [-1, 10, 26, 26]               0\n",
            "            Conv2d-5           [-1, 16, 24, 24]           1,440\n",
            "              ReLU-6           [-1, 16, 24, 24]               0\n",
            "       BatchNorm2d-7           [-1, 16, 24, 24]              32\n",
            "           Dropout-8           [-1, 16, 24, 24]               0\n",
            "            Conv2d-9           [-1, 10, 24, 24]             160\n",
            "        MaxPool2d-10           [-1, 10, 12, 12]               0\n",
            "           Conv2d-11           [-1, 16, 10, 10]           1,440\n",
            "             ReLU-12           [-1, 16, 10, 10]               0\n",
            "      BatchNorm2d-13           [-1, 16, 10, 10]              32\n",
            "          Dropout-14           [-1, 16, 10, 10]               0\n",
            "           Conv2d-15             [-1, 16, 8, 8]           2,304\n",
            "             ReLU-16             [-1, 16, 8, 8]               0\n",
            "      BatchNorm2d-17             [-1, 16, 8, 8]              32\n",
            "          Dropout-18             [-1, 16, 8, 8]               0\n",
            "        AvgPool2d-19             [-1, 16, 1, 1]               0\n",
            "           Conv2d-20             [-1, 10, 1, 1]             160\n",
            "================================================================\n",
            "Total params: 5,710\n",
            "Trainable params: 5,710\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.62\n",
            "Params size (MB): 0.02\n",
            "Estimated Total Size (MB): 0.65\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=0 Loss=0.3178 Batch=468 Accuracy=70.76: 100%|██████████| 469/469 [00:44<00:00, 10.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.1546, Accuracy: 9563/10000 (95.63%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=1 Loss=0.1458 Batch=468 Accuracy=93.32: 100%|██████████| 469/469 [00:43<00:00, 10.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0799, Accuracy: 9751/10000 (97.51%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=2 Loss=0.1506 Batch=468 Accuracy=95.54: 100%|██████████| 469/469 [00:44<00:00, 10.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0427, Accuracy: 9859/10000 (98.59%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=3 Loss=0.1713 Batch=468 Accuracy=96.15: 100%|██████████| 469/469 [00:43<00:00, 10.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0454, Accuracy: 9852/10000 (98.52%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=4 Loss=0.0489 Batch=468 Accuracy=96.66: 100%|██████████| 469/469 [00:43<00:00, 10.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0404, Accuracy: 9883/10000 (98.83%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=5 Loss=0.1476 Batch=468 Accuracy=96.92: 100%|██████████| 469/469 [00:43<00:00, 10.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0342, Accuracy: 9889/10000 (98.89%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=6 Loss=0.1209 Batch=468 Accuracy=97.14: 100%|██████████| 469/469 [00:44<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0308, Accuracy: 9903/10000 (99.03%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=7 Loss=0.0686 Batch=468 Accuracy=97.25: 100%|██████████| 469/469 [00:44<00:00, 10.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0314, Accuracy: 9901/10000 (99.01%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=8 Loss=0.0311 Batch=468 Accuracy=97.39: 100%|██████████| 469/469 [00:44<00:00, 10.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0208, Accuracy: 9933/10000 (99.33%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=9 Loss=0.0479 Batch=468 Accuracy=97.61: 100%|██████████| 469/469 [00:44<00:00, 10.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0233, Accuracy: 9910/10000 (99.10%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=10 Loss=0.1047 Batch=468 Accuracy=97.73: 100%|██████████| 469/469 [00:43<00:00, 10.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0203, Accuracy: 9925/10000 (99.25%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=11 Loss=0.0405 Batch=468 Accuracy=97.91: 100%|██████████| 469/469 [00:43<00:00, 10.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0185, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=12 Loss=0.0411 Batch=468 Accuracy=98.04: 100%|██████████| 469/469 [00:43<00:00, 10.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0180, Accuracy: 9936/10000 (99.36%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=13 Loss=0.0400 Batch=468 Accuracy=98.12: 100%|██████████| 469/469 [00:43<00:00, 10.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 9934/10000 (99.34%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch=14 Loss=0.0977 Batch=468 Accuracy=98.08: 100%|██████████| 469/469 [00:43<00:00, 10.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.0172, Accuracy: 9932/10000 (99.32%)\n",
            "\n",
            "Times achieved ≥99.4%: 0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # Initial Block: 28x28x1 -> 26x26x10\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 10, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(10),\n",
        "            nn.Dropout(0.005)  # Slightly increased dropout\n",
        "        )\n",
        "\n",
        "        # Conv Block 1: 26x26x10 -> 24x24x16\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        # Transition Block: 24x24x16 -> 12x12x10\n",
        "        self.trans1 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, kernel_size=1, bias=False),\n",
        "            nn.MaxPool2d(2, 2)\n",
        "        )\n",
        "\n",
        "        # Conv Block 2: 12x12x10 -> 10x10x16\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(10, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        # Conv Block 3: 10x10x16 -> 8x8x16\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=0, bias=False),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout(0.005)\n",
        "        )\n",
        "\n",
        "        self.gap = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=8)\n",
        "        )\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(16, 10, kernel_size=1, padding=0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.trans1(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        x = self.gap(x)\n",
        "        x = self.conv5(x)\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "def train(model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    correct = 0\n",
        "    processed = 0\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(data)\n",
        "        loss = F.nll_loss(y_pred, target)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        pred = y_pred.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "        processed += len(data)\n",
        "\n",
        "        pbar.set_description(desc=f'Epoch={epoch} Loss={loss.item():.4f} Batch={batch_idx} Accuracy={100*correct/processed:.2f}')\n",
        "    return 100 * correct / processed\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100.*correct/len(test_loader.dataset):.2f}%)\\n')\n",
        "    return 100. * correct / len(test_loader.dataset)\n",
        "\n",
        "def main():\n",
        "    torch.manual_seed(1)\n",
        "    batch_size = 128\n",
        "    epochs = 15\n",
        "\n",
        "    # Enhanced data augmentation with more aggressive transformations\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomRotation((-12.0, 12.0)),  # Increased rotation\n",
        "        transforms.RandomAffine(\n",
        "            degrees=0,\n",
        "            translate=(0.13, 0.13),  # Increased translation\n",
        "            scale=(0.82, 1.18),      # Increased scale range\n",
        "            shear=(-3, 3)            # Added shear\n",
        "        ),\n",
        "        transforms.ColorJitter(\n",
        "            brightness=0.15,          # Increased brightness adjustment\n",
        "            contrast=0.15            # Increased contrast adjustment\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,)),\n",
        "        transforms.RandomErasing(p=0.2, scale=(0.02, 0.1))  # Added random erasing\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=True, download=True, transform=train_transform),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        datasets.MNIST('../data', train=False, transform=test_transform),\n",
        "        batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True\n",
        "    )\n",
        "\n",
        "\n",
        "    from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = Net().to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=0.015,\n",
        "        epochs=epochs,\n",
        "        steps_per_epoch=len(train_loader),\n",
        "        pct_start=0.15,\n",
        "        div_factor=15,\n",
        "        final_div_factor=100\n",
        "    )\n",
        "\n",
        "    summary(model, input_size=(1, 28, 28))\n",
        "\n",
        "    accuracies_above_99_4 = 0\n",
        "    best_acc = 0\n",
        "    for epoch in range(epochs):\n",
        "        train_acc = train(model, device, train_loader, optimizer, scheduler, epoch)\n",
        "        test_acc = test(model, device, test_loader)\n",
        "        if test_acc >= 99.4:\n",
        "            accuracies_above_99_4 += 1\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "        print(f\"Times achieved ≥99.4%: {accuracies_above_99_4}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}